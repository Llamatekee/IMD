{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 2: Clasificación y evaluación de modelos\n",
        "Duración: 1 sesión\n",
        "\n",
        "Apartado 2.3: Clasificación usando agrupaciones de clasificadores\n",
        "\n",
        "Objetivo: El objetivo de esta práctica es introducir los conceptos de clasificación usando agrupaciones (ensembles) de clasificadores. Usaremos dos clasificadores diferentes para ver cómo se comportan como métodos base de un ensemble.\n",
        "\n",
        "Realice los siguientes ejercicios usando el módulo scikit-learn de Python y cualquier otro módulo adicional que considere utilizando los mismo conjuntos de datos de la práctica 3:\n",
        "1. Seleccione como métodos base un árbol de decisión y una máquina de vectores soporte.\n",
        "2. Para cada uno de estos dos métodos de clasificación realice los siguientes pasos usando validación cruzada de 10\n",
        "particiones:\n",
        " 1. Aplique el método base a cada uno de los conjuntos y anote los resultados obtenidos.\n",
        " 2. Aplique el método de combinación de clasificadores Bagging a cada uno de los conjuntos y anote los\n",
        "resultados obtenidos.\n",
        " 3. Seleccione dos algoritmos de Boosting y aplique estos algoritmos a cada uno de los conjuntos y anote los\n",
        "resultados obtenidos.\n",
        " 4. Compare si hay diferencias significativas entre ellos usando el test de Iman-Davenport. Si es así, aplique el\n",
        "procedimiento de Wilcoxon para comparar cada método de agrupación con el clasificador base.\n",
        "3. Enuncie las conclusiones del estudio indicando la influencia del clasificador base en el rendimiento de las\n",
        "agrupaciones de clasificadores.\n",
        "4. Evalúe la influencia de la validación de híper-parámetros en el rendimiento de los ensembles con respecto a\n",
        "los métodos base."
      ],
      "metadata": {
        "id": "pWFrzTV1LbFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn-evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwqfwzmTM86A",
        "outputId": "f95ac787-9c73-405f-db7d-0b796539a33c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn-evaluation in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: ploomber-core>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.2.25)\n",
            "Requirement already satisfied: ploomber-extension in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (3.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (4.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (3.1.3)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.8.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (1.5.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (5.9.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (7.34.0)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (24.1.1)\n",
            "Requirement already satisfied: parso in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.8.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ploomber-core>=0.2.6->sklearn-evaluation) (6.0.1)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from ploomber-core>=0.2.6->sklearn-evaluation) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (23.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (4.9.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.19.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->sklearn-evaluation) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (1.23.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->sklearn-evaluation) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->sklearn-evaluation) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->sklearn-evaluation) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sklearn-evaluation) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-evaluation) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-evaluation) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-evaluation) (3.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (0.17.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->sklearn-evaluation) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->sklearn-evaluation) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->sklearn-evaluation) (1.16.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2.31.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->ploomber-core>=0.2.6->sklearn-evaluation) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "#Disable warning of Ripper implementation\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn import tree, svm\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn_evaluation import plot\n",
        "import seaborn as sns\n",
        "import time"
      ],
      "metadata": {
        "id": "Oiy2mycMLXF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "data = arff.loadarff('ionosphere.arff')\n",
        "df_iono = pd.DataFrame(data[0])\n",
        "\n",
        "data = arff.loadarff('diabetes.arff')\n",
        "df_diabe = pd.DataFrame(data[0])\n",
        "\n",
        "data = arff.loadarff('vehicle.arff')\n",
        "df_Vehicle = pd.DataFrame(data[0])\n",
        "\n",
        "data = arff.loadarff('vowel.arff')\n",
        "df_vowel = pd.DataFrame(data[0])\n",
        "\n",
        "data = arff.loadarff('iris.arff')\n",
        "df_iris = pd.DataFrame(data[0])\n",
        "\n",
        "data = arff.loadarff('letter.arff')\n",
        "df_letter = pd.DataFrame(data[0])"
      ],
      "metadata": {
        "id": "r-y2F7f5LXF9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "# Changing the last categorical class value into a numerical value\n",
        "df_iono['class'] = pd.factorize(df_iono['class'])[0]\n",
        "\n",
        "# Changing the last categorical class value into a numerical value\n",
        "df_diabe['class'] = pd.factorize(df_diabe['class'])[0]\n",
        "\n",
        "# Changing the last categorical class value into a numerical value\n",
        "df_Vehicle['Class'] = pd.factorize(df_Vehicle['Class'])[0]\n",
        "\n",
        "# Changing the last categorical class value into a numerical value\n",
        "df_vowel['Class'] = pd.factorize(df_vowel['Class'])[0]\n",
        "\n",
        "# Changing the last categorical class value into a numerical value\n",
        "df_iris['class'] = pd.factorize(df_iris['class'])[0]\n",
        "\n",
        "# Changing the last categorical class value into a numerical value\n",
        "df_letter['class'] = pd.factorize(df_letter['class'])[0]\n"
      ],
      "metadata": {
        "id": "BH3PDtR3LXF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "trainIono, testIono = train_test_split(df_iono, test_size=.25)\n",
        "X_trainIono = trainIono.drop('class', axis=1)\n",
        "y_trainIono = trainIono['class']\n",
        "X_testIono = testIono.drop('class', axis=1)\n",
        "y_testIono = testIono['class']\n",
        "\n",
        "trainDiabe, testDiabe = train_test_split(df_diabe, test_size=.25)\n",
        "X_trainDiabe = trainDiabe.drop('class', axis=1)\n",
        "y_trainDiabe = trainDiabe['class']\n",
        "X_testDiabe = testDiabe.drop('class', axis=1)\n",
        "y_testDiabe = testDiabe['class']\n",
        "\n",
        "trainVehicle, testVehicle = train_test_split(df_Vehicle, test_size=.25)\n",
        "X_trainVehicle = trainVehicle.drop('Class', axis=1)\n",
        "y_trainVehicle = trainVehicle['Class']\n",
        "X_testVehicle = testVehicle.drop('Class', axis=1)\n",
        "y_testVehicle = testVehicle['Class']\n",
        "\n",
        "trainVowel, testVowel = train_test_split(df_vowel, test_size=.25)\n",
        "X_trainVowel = trainVowel.drop('Class', axis=1)\n",
        "y_trainVowel = trainVowel['Class']\n",
        "X_testVowel = testVowel.drop('Class', axis=1)\n",
        "y_testVowel = testVowel['Class']\n",
        "\n",
        "trainIris, testIris = train_test_split(df_iris, test_size=.25)\n",
        "X_trainIris = trainIris.drop('class', axis=1)\n",
        "y_trainIris = trainIris['class']\n",
        "X_testIris = testIris.drop('class', axis=1)\n",
        "y_testIris = testIris['class']\n",
        "\n",
        "trainLetter, testLetter = train_test_split(df_letter, test_size=.25)\n",
        "X_trainLetter = trainLetter.drop('class', axis=1)\n",
        "y_trainLetter = trainLetter['class']\n",
        "X_testLetter = testLetter.drop('class', axis=1)\n",
        "y_testLetter = testLetter['class']"
      ],
      "metadata": {
        "id": "QFD_yQncLXF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "data = [\n",
        "    ('Ionosphere', X_trainIono, y_trainIono, X_testIono, y_testIono),\n",
        "    ('Diabetes', X_trainDiabe, y_trainDiabe,X_testDiabe, y_testDiabe),\n",
        "    ('Vehicle', X_trainVehicle, y_trainVehicle, X_testVehicle, y_testVehicle),\n",
        "    ('Vowel', X_trainVowel, y_trainVowel, X_testVowel, y_testVowel),\n",
        "    ('Iris', X_trainIris, y_trainIris, X_testIris, y_testIris)\n",
        "]"
      ],
      "metadata": {
        "id": "GurDG10ILXF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison without using GridSearch"
      ],
      "metadata": {
        "collapsed": false,
        "id": "WjHHe1KTLXF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "clf_tree = tree.DecisionTreeClassifier()\n",
        "clf_svm = svm.SVC()\n",
        "clf_BaggingTree = BaggingClassifier(estimator=clf_tree)\n",
        "clf_AdaBoostSAMMETree = AdaBoostClassifier(estimator=clf_tree,algorithm='SAMME')\n",
        "clf_AdaBoostSAMMERTree = AdaBoostClassifier(estimator=clf_tree,algorithm='SAMME.R')\n",
        "clf_BaggingSVM = BaggingClassifier(estimator=clf_svm)\n",
        "clf_AdaBoostSAMMESVM = AdaBoostClassifier(estimator=clf_svm,algorithm='SAMME')\n",
        "clf_AdaBoostSAMMERSVM = AdaBoostClassifier(estimator=clf_svm,algorithm='SAMME.R')\n",
        "clf_GradBoost = GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "4tpy1d2iLXGA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9354700854700855 +- 0.041320609835001056\n",
            "Tiempo medio en ejecutarse el método (train): 0.011040067672729493 +- 0.011040067672729493s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0065200567245483395 +- 0.0065200567245483395s\n",
            "Tiempo en ejecutarse la búsqueda 2.694411039352417s, (0.04490685065587362 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7673018753781004 +- 0.02396020444414765\n",
            "Tiempo medio en ejecutarse el método (train): 0.020153498649597167 +- 0.020153498649597167s\n",
            "Tiempo medio en ejecutarse el método (score): 0.006453418731689453 +- 0.006453418731689453s\n",
            "Tiempo en ejecutarse la búsqueda 0.16493487358093262s, (0.00274891455968221 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.774454365079365 +- 0.029626676912587448\n",
            "Tiempo medio en ejecutarse el método (train): 0.023032498359680176 +- 0.023032498359680176s\n",
            "Tiempo medio en ejecutarse el método (score): 0.00799558162689209 +- 0.00799558162689209s\n",
            "Tiempo en ejecutarse la búsqueda 0.192352294921875s, (0.00320587158203125 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.964918918918919 +- 0.012362789664878568\n",
            "Tiempo medio en ejecutarse el método (train): 0.012998652458190919 +- 0.012998652458190919s\n",
            "Tiempo medio en ejecutarse el método (score): 0.005044198036193848 +- 0.005044198036193848s\n",
            "Tiempo en ejecutarse la búsqueda 0.11707448959350586s, (0.0019512414932250977 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9818181818181818 +- 0.036363636363636376\n",
            "Tiempo medio en ejecutarse el método (train): 0.005596661567687988 +- 0.005596661567687988s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004672098159790039 +- 0.004672098159790039s\n",
            "Tiempo en ejecutarse la búsqueda 0.07512187957763672s, (0.0012520313262939452 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.011040 &   0.006520 &  2.694411 &  0.935470 \\\\\n",
            "1 &    Diabetes &   0.020153 &   0.006453 &  0.164935 &  0.767302 \\\\\n",
            "2 &     Vehicle &   0.023032 &   0.007996 &  0.192352 &  0.774454 \\\\\n",
            "3 &       Vowel &   0.012999 &   0.005044 &  0.117074 &  0.964919 \\\\\n",
            "4 &        Iris &   0.005597 &   0.004672 &  0.075122 &  0.981818 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreSVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_svm,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreSVM.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreSVM)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixYqVbL0LXGA",
        "outputId": "8fd48ac9-6d3b-47a3-9191-0a4e23f59acf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.8477207977207977 +- 0.06365290213770579\n",
            "Tiempo medio en ejecutarse el método (train): 0.011526870727539062 +- 0.011526870727539062s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004701423645019531 +- 0.004701423645019531s\n",
            "Tiempo en ejecutarse la búsqueda 0.13386106491088867s, (0.0022310177485148114 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7189655172413792 +- 0.059408008699639664\n",
            "Tiempo medio en ejecutarse el método (train): 0.008754444122314454 +- 0.008754444122314454s\n",
            "Tiempo medio en ejecutarse el método (score): 0.003648829460144043 +- 0.003648829460144043s\n",
            "Tiempo en ejecutarse la búsqueda 0.09942007064819336s, (0.0016570011774698892 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9179811507936508 +- 0.01972094280306819\n",
            "Tiempo medio en ejecutarse el método (train): 0.00759437084197998 +- 0.00759437084197998s\n",
            "Tiempo medio en ejecutarse el método (score): 0.003861093521118164 +- 0.003861093521118164s\n",
            "Tiempo en ejecutarse la búsqueda 0.09325027465820312s, (0.0015541712443033855 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9811171171171171 +- 0.013760340424212333\n",
            "Tiempo medio en ejecutarse el método (train): 0.009406065940856934 +- 0.009406065940856934s\n",
            "Tiempo medio en ejecutarse el método (score): 0.003616023063659668 +- 0.003616023063659668s\n",
            "Tiempo en ejecutarse la búsqueda 0.10506391525268555s, (0.0017510652542114258 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9462121212121213 +- 0.05988683083021719\n",
            "Tiempo medio en ejecutarse el método (train): 0.004551076889038086 +- 0.004551076889038086s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0030878543853759765 +- 0.0030878543853759765s\n",
            "Tiempo en ejecutarse la búsqueda 0.07017755508422852s, (0.0011696259180704752 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.011527 &   0.004701 &  0.133861 &  0.847721 \\\\\n",
            "1 &    Diabetes &   0.008754 &   0.003649 &  0.099420 &  0.718966 \\\\\n",
            "2 &     Vehicle &   0.007594 &   0.003861 &  0.093250 &  0.917981 \\\\\n",
            "3 &       Vowel &   0.009406 &   0.003616 &  0.105064 &  0.981117 \\\\\n",
            "4 &        Iris &   0.004551 &   0.003088 &  0.070178 &  0.946212 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreTree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_tree,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreTree.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreTree)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4HFy1S9LXGB",
        "outputId": "622b0f37-92cc-4e3f-ab1b-5a06fe3db170"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9316239316239316 +- 0.04049694954010985\n",
            "Tiempo medio en ejecutarse el método (train): 0.07846007347106934 +- 0.07846007347106934s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0173443078994751 +- 0.0173443078994751s\n",
            "Tiempo en ejecutarse la búsqueda 0.555586576461792s, (0.0092597762743632 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7637326073805203 +- 0.03916681240522346\n",
            "Tiempo medio en ejecutarse el método (train): 0.11821725368499755 +- 0.11821725368499755s\n",
            "Tiempo medio en ejecutarse el método (score): 0.020959067344665527 +- 0.020959067344665527s\n",
            "Tiempo en ejecutarse la búsqueda 0.7479491233825684s, (0.012465818723042806 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7681795634920634 +- 0.02179551582301958\n",
            "Tiempo medio en ejecutarse el método (train): 0.11349108219146728 +- 0.11349108219146728s\n",
            "Tiempo medio en ejecutarse el método (score): 0.024461007118225096 +- 0.024461007118225096s\n",
            "Tiempo en ejecutarse la búsqueda 0.7807338237762451s, (0.013012230396270752 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9703243243243242 +- 0.011732220435522838\n",
            "Tiempo medio en ejecutarse el método (train): 0.09165759086608886 +- 0.09165759086608886s\n",
            "Tiempo medio en ejecutarse el método (score): 0.01871492862701416 +- 0.01871492862701416s\n",
            "Tiempo en ejecutarse la búsqueda 0.6205992698669434s, (0.010343321164449056 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9727272727272727 +- 0.058210220340298616\n",
            "Tiempo medio en ejecutarse el método (train): 0.03841001987457275 +- 0.03841001987457275s\n",
            "Tiempo medio en ejecutarse el método (score): 0.007108807563781738 +- 0.007108807563781738s\n",
            "Tiempo en ejecutarse la búsqueda 0.26035332679748535s, (0.0043392221132914225 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.078460 &   0.017344 &  0.555587 &  0.931624 \\\\\n",
            "1 &    Diabetes &   0.118217 &   0.020959 &  0.747949 &  0.763733 \\\\\n",
            "2 &     Vehicle &   0.113491 &   0.024461 &  0.780734 &  0.768180 \\\\\n",
            "3 &       Vowel &   0.091658 &   0.018715 &  0.620599 &  0.970324 \\\\\n",
            "4 &        Iris &   0.038410 &   0.007109 &  0.260353 &  0.972727 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreBaggingSVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_BaggingSVM,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreBaggingSVM.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreBaggingSVM)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFWzA6DsLXGB",
        "outputId": "6ea3b68b-2922-47bc-a6d8-f9bdb7c090b9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.8934472934472936 +- 0.04469757704713748\n",
            "Tiempo medio en ejecutarse el método (train): 0.11371309757232666 +- 0.11371309757232666s\n",
            "Tiempo medio en ejecutarse el método (score): 0.009350419044494629 +- 0.009350419044494629s\n",
            "Tiempo en ejecutarse la búsqueda 0.7013664245605469s, (0.011689440409342448 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7290986085904416 +- 0.049147647336200355\n",
            "Tiempo medio en ejecutarse el método (train): 0.22492485046386718 +- 0.22492485046386718s\n",
            "Tiempo medio en ejecutarse el método (score): 0.023783040046691895 +- 0.023783040046691895s\n",
            "Tiempo en ejecutarse la búsqueda 1.3449058532714844s, (0.02241509755452474 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.941641865079365 +- 0.015823354387363323\n",
            "Tiempo medio en ejecutarse el método (train): 0.20932316780090332 +- 0.20932316780090332s\n",
            "Tiempo medio en ejecutarse el método (score): 0.019408631324768066 +- 0.019408631324768066s\n",
            "Tiempo en ejecutarse la búsqueda 1.237889289855957s, (0.02063148816426595 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9878558558558558 +- 0.011223128229504673\n",
            "Tiempo medio en ejecutarse el método (train): 0.17121920585632325 +- 0.17121920585632325s\n",
            "Tiempo medio en ejecutarse el método (score): 0.025694561004638673 +- 0.025694561004638673s\n",
            "Tiempo en ejecutarse la búsqueda 1.0953021049499512s, (0.018255035082499187 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9469696969696969 +- 0.059458839001056314\n",
            "Tiempo medio en ejecutarse el método (train): 0.06697983741760254 +- 0.06697983741760254s\n",
            "Tiempo medio en ejecutarse el método (score): 0.011137700080871582 +- 0.011137700080871582s\n",
            "Tiempo en ejecutarse la búsqueda 0.46090197563171387s, (0.007681699593861898 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.113713 &   0.009350 &  0.701366 &  0.893447 \\\\\n",
            "1 &    Diabetes &   0.224925 &   0.023783 &  1.344906 &  0.729099 \\\\\n",
            "2 &     Vehicle &   0.209323 &   0.019409 &  1.237889 &  0.941642 \\\\\n",
            "3 &       Vowel &   0.171219 &   0.025695 &  1.095302 &  0.987856 \\\\\n",
            "4 &        Iris &   0.066980 &   0.011138 &  0.460902 &  0.946970 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreBaggingTree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_BaggingTree,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreBaggingTree.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreBaggingTree)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdzEj---LXGC",
        "outputId": "d1d6bf65-b471-455c-9543-4c9a17f4b655"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.6427350427350428 +- 0.01415641965675765\n",
            "Tiempo medio en ejecutarse el método (train): 1.579045820236206 +- 1.579045820236206s\n",
            "Tiempo medio en ejecutarse el método (score): 0.11202099323272705 +- 0.11202099323272705s\n",
            "Tiempo en ejecutarse la búsqueda 8.724980115890503s, (0.1454163352648417 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.6614942528735632 +- 0.006527480857241695\n",
            "Tiempo medio en ejecutarse el método (train): 1.853383684158325 +- 1.853383684158325s\n",
            "Tiempo medio en ejecutarse el método (score): 0.12757401466369628 +- 0.12757401466369628s\n",
            "Tiempo en ejecutarse la búsqueda 11.37070918083191s, (0.18951181968053182 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7649801587301587 +- 0.004612295117503322\n",
            "Tiempo medio en ejecutarse el método (train): 0.6916273832321167 +- 0.6916273832321167s\n",
            "Tiempo medio en ejecutarse el método (score): 0.03949859142303467 +- 0.03949859142303467s\n",
            "Tiempo en ejecutarse la búsqueda 5.358199834823608s, (0.08930333058039347 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9055315315315315 +- 0.0003783783783783745\n",
            "Tiempo medio en ejecutarse el método (train): 3.219229292869568 +- 3.219229292869568s\n",
            "Tiempo medio en ejecutarse el método (score): 0.23379218578338623 +- 0.23379218578338623s\n",
            "Tiempo en ejecutarse la búsqueda 17.32469415664673s, (0.2887449026107788 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.3484848484848485 +- 0.027938013506948155\n",
            "Tiempo medio en ejecutarse el método (train): 0.17798595428466796 +- 0.17798595428466796s\n",
            "Tiempo medio en ejecutarse el método (score): 0.017029786109924318 +- 0.017029786109924318s\n",
            "Tiempo en ejecutarse la búsqueda 1.0687270164489746s, (0.017812116940816244 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &     TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   1.579046 &   0.112021 &   8.724980 &  0.642735 \\\\\n",
            "1 &    Diabetes &   1.853384 &   0.127574 &  11.370709 &  0.661494 \\\\\n",
            "2 &     Vehicle &   0.691627 &   0.039499 &   5.358200 &  0.764980 \\\\\n",
            "3 &       Vowel &   3.219229 &   0.233792 &  17.324694 &  0.905532 \\\\\n",
            "4 &        Iris &   0.177986 &   0.017030 &   1.068727 &  0.348485 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreBoosting1_SVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_AdaBoostSAMMESVM,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreBoosting1_SVM.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreBoosting1_SVM)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OXJDcp-LXGC",
        "outputId": "e12461dc-7283-4102-8b28-651f041355b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.8371794871794871 +- 0.07039703867375144\n",
            "Tiempo medio en ejecutarse el método (train): 0.015830302238464357 +- 0.015830302238464357s\n",
            "Tiempo medio en ejecutarse el método (score): 0.00475924015045166 +- 0.00475924015045166s\n",
            "Tiempo en ejecutarse la búsqueda 0.13798165321350098s, (0.0022996942202250163 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7310647307924986 +- 0.049429850267089084\n",
            "Tiempo medio en ejecutarse el método (train): 0.011345744132995605 +- 0.011345744132995605s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0046727657318115234 +- 0.0046727657318115234s\n",
            "Tiempo en ejecutarse la búsqueda 0.1191854476928711s, (0.001986424128214518 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9211061507936508 +- 0.021341027288853148\n",
            "Tiempo medio en ejecutarse el método (train): 0.010860872268676759 +- 0.010860872268676759s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004534339904785157 +- 0.004534339904785157s\n",
            "Tiempo en ejecutarse la búsqueda 0.11176753044128418s, (0.001862792174021403 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9811171171171171 +- 0.010784291798641528\n",
            "Tiempo medio en ejecutarse el método (train): 0.011528301239013671 +- 0.011528301239013671s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004052352905273437 +- 0.004052352905273437s\n",
            "Tiempo en ejecutarse la búsqueda 0.10511922836303711s, (0.0017519871393839519 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9553030303030303 +- 0.060459102129481135\n",
            "Tiempo medio en ejecutarse el método (train): 0.006727337837219238 +- 0.006727337837219238s\n",
            "Tiempo medio en ejecutarse el método (score): 0.003338146209716797 +- 0.003338146209716797s\n",
            "Tiempo en ejecutarse la búsqueda 0.07315897941589355s, (0.001219316323598226 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.015830 &   0.004759 &  0.137982 &  0.837179 \\\\\n",
            "1 &    Diabetes &   0.011346 &   0.004673 &  0.119185 &  0.731065 \\\\\n",
            "2 &     Vehicle &   0.010861 &   0.004534 &  0.111768 &  0.921106 \\\\\n",
            "3 &       Vowel &   0.011528 &   0.004052 &  0.105119 &  0.981117 \\\\\n",
            "4 &        Iris &   0.006727 &   0.003338 &  0.073159 &  0.955303 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreBoosting1_Tree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_AdaBoostSAMMETree,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreBoosting1_Tree.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreBoosting1_Tree)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRB3afK3LXGD",
        "outputId": "12098529-eb2d-4d7c-9b18-9075f07b29ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost with SVM and SAMMER_R is not supported"
      ],
      "metadata": {
        "collapsed": false,
        "id": "JvI_sRmCLXGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDatasets = []\\nTimeTrain = []\\nTimeScore = []\\nTimeCV = []\\nScoreBoosting2_SVM = []\\nfor i in data:\\n    init = time.time()\\n    cv = cross_validate(clf_AdaBoostSAMMERSVM,i[1], y=i[2],cv=10, n_jobs=-1, error_score=\\'raise\\')\\n    end = time.time()\\n    timeCV = end - init\\n    print(f\"\\n--------- {i[0]} ---------\")\\n    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv[\\'test_score\\'])} +- {np.std(cv[\\'test_score\\'])}\")\\n    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv[\\'fit_time\\'])} +- {np.mean(np.mean(cv[\\'fit_time\\']))}s\")\\n    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv[\\'score_time\\'])} +- {np.mean(np.mean(cv[\\'score_time\\']))}s\")\\n    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\\n    Datasets.append(i[0])\\n    TimeTrain.append(np.mean(cv[\\'fit_time\\']))\\n    TimeScore.append(np.mean(cv[\\'score_time\\']))\\n    TimeCV.append(timeCV)\\n    ScoreBoosting2_SVM.append(np.mean(cv[\\'test_score\\']))\\n\\nmy_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=Score)\\nSVMDF = pd.DataFrame (my_dict)\\nprint(SVMDF.to_latex())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\"\"\"\n",
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreBoosting2_SVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_AdaBoostSAMMERSVM,i[1], y=i[2],cv=10, n_jobs=-1, error_score='raise')\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreBoosting2_SVM.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=Score)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "X7kMp0gTLXGD",
        "outputId": "baa46741-e62a-4497-99b4-bec9be445402"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.8254985754985755 +- 0.06478663173621244\n",
            "Tiempo medio en ejecutarse el método (train): 0.014299845695495606 +- 0.014299845695495606s\n",
            "Tiempo medio en ejecutarse el método (score): 0.005156207084655762 +- 0.005156207084655762s\n",
            "Tiempo en ejecutarse la búsqueda 0.12090253829956055s, (0.002015042304992676 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.722292800967937 +- 0.04596973746069332\n",
            "Tiempo medio en ejecutarse el método (train): 0.009687232971191406 +- 0.009687232971191406s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0034607887268066407 +- 0.0034607887268066407s\n",
            "Tiempo en ejecutarse la búsqueda 0.09449315071105957s, (0.0015748858451843262 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.925843253968254 +- 0.01743817928244871\n",
            "Tiempo medio en ejecutarse el método (train): 0.011609244346618652 +- 0.011609244346618652s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004069828987121582 +- 0.004069828987121582s\n",
            "Tiempo en ejecutarse la búsqueda 0.11439013481140137s, (0.0019065022468566894 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9797477477477479 +- 0.010906245432442636\n",
            "Tiempo medio en ejecutarse el método (train): 0.010727024078369141 +- 0.010727024078369141s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0037964820861816407 +- 0.0037964820861816407s\n",
            "Tiempo en ejecutarse la búsqueda 0.10193872451782227s, (0.0016989787419637045 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9553030303030303 +- 0.060459102129481135\n",
            "Tiempo medio en ejecutarse el método (train): 0.008019638061523438 +- 0.008019638061523438s\n",
            "Tiempo medio en ejecutarse el método (score): 0.0055264711380004885 +- 0.0055264711380004885s\n",
            "Tiempo en ejecutarse la búsqueda 0.08953642845153809s, (0.0014922738075256348 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.014300 &   0.005156 &  0.120903 &  0.825499 \\\\\n",
            "1 &    Diabetes &   0.009687 &   0.003461 &  0.094493 &  0.722293 \\\\\n",
            "2 &     Vehicle &   0.011609 &   0.004070 &  0.114390 &  0.925843 \\\\\n",
            "3 &       Vowel &   0.010727 &   0.003796 &  0.101939 &  0.979748 \\\\\n",
            "4 &        Iris &   0.008020 &   0.005526 &  0.089536 &  0.955303 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreBoosting2_Tree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_AdaBoostSAMMERTree,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreBoosting2_Tree.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreBoosting2_Tree)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT-UpHouLXGD",
        "outputId": "4cd0f584-2287-4a03-b53d-e97c0d5038a8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9086894586894587 +- 0.0392265896430168\n",
            "Tiempo medio en ejecutarse el método (train): 0.3920912265777588 +- 0.3920912265777588s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004993057250976563 +- 0.004993057250976563s\n",
            "Tiempo en ejecutarse la búsqueda 2.0352988243103027s, (0.03392164707183838 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.7551421657592257 +- 0.04092040285321942\n",
            "Tiempo medio en ejecutarse el método (train): 0.23976457118988037 +- 0.23976457118988037s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004790067672729492 +- 0.004790067672729492s\n",
            "Tiempo en ejecutarse la búsqueda 1.2541770935058594s, (0.02090295155843099 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9557787698412697 +- 0.028276430973572984\n",
            "Tiempo medio en ejecutarse el método (train): 0.5351858377456665 +- 0.5351858377456665s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004751372337341309 +- 0.004751372337341309s\n",
            "Tiempo en ejecutarse la búsqueda 2.838075637817383s, (0.047301260630289714 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9878738738738738 +- 0.012706995632653635\n",
            "Tiempo medio en ejecutarse el método (train): 0.5321404933929443 +- 0.5321404933929443s\n",
            "Tiempo medio en ejecutarse el método (score): 0.003966712951660156 +- 0.003966712951660156s\n",
            "Tiempo en ejecutarse la búsqueda 2.771550416946411s, (0.04619250694910685 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "Score de la Validación Cruzada:\n",
            "   score = 0.9734848484848484 +- 0.04054976876082619\n",
            "Tiempo medio en ejecutarse el método (train): 0.42891085147857666 +- 0.42891085147857666s\n",
            "Tiempo medio en ejecutarse el método (score): 0.004716944694519043 +- 0.004716944694519043s\n",
            "Tiempo en ejecutarse la búsqueda 2.2290585041046143s, (0.03715097506841024 min)\n",
            "\\begin{tabular}{llrrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeTrain &  TimeScore &    TimeCV &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &   0.392091 &   0.004993 &  2.035299 &  0.908689 \\\\\n",
            "1 &    Diabetes &   0.239765 &   0.004790 &  1.254177 &  0.755142 \\\\\n",
            "2 &     Vehicle &   0.535186 &   0.004751 &  2.838076 &  0.955779 \\\\\n",
            "3 &       Vowel &   0.532140 &   0.003967 &  2.771550 &  0.987874 \\\\\n",
            "4 &        Iris &   0.428911 &   0.004717 &  2.229059 &  0.973485 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeTrain = []\n",
        "TimeScore = []\n",
        "TimeCV = []\n",
        "ScoreGradBoost = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    cv = cross_validate(clf_GradBoost,i[1], y=i[2],cv=10, n_jobs=-1)\n",
        "    end = time.time()\n",
        "    timeCV = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"Score de la Validación Cruzada:\\n   score = {np.mean(cv['test_score'])} +- {np.std(cv['test_score'])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (train): {np.mean(cv['fit_time'])} +- {np.mean(np.mean(cv['fit_time']))}s\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método (score): {np.mean(cv['score_time'])} +- {np.mean(np.mean(cv['score_time']))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeCV}s, ({timeCV/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeTrain.append(np.mean(cv['fit_time']))\n",
        "    TimeScore.append(np.mean(cv['score_time']))\n",
        "    TimeCV.append(timeCV)\n",
        "    ScoreGradBoost.append(np.mean(cv['test_score']))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeTrain=TimeTrain, TimeScore=TimeScore,TimeCV=TimeCV, Score=ScoreGradBoost)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3DUm9C3LXGE",
        "outputId": "2916dd0e-38a3-467a-c12d-95753e11c60c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        SVM      Tree  BaggingSVM  BaggingTree  Boosting1_SVM  Boosting1_Tree  \\\n",
            "0  0.935470  0.847721    0.931624     0.893447       0.642735        0.837179   \n",
            "1  0.767302  0.718966    0.763733     0.729099       0.661494        0.731065   \n",
            "2  0.774454  0.917981    0.768180     0.941642       0.764980        0.921106   \n",
            "3  0.964919  0.981117    0.970324     0.987856       0.905532        0.981117   \n",
            "4  0.981818  0.946212    0.972727     0.946970       0.348485        0.955303   \n",
            "\n",
            "   Boosting2_Tree  GradBoost  \n",
            "0        0.825499   0.908689  \n",
            "1        0.722293   0.755142  \n",
            "2        0.925843   0.955779  \n",
            "3        0.979748   0.987874  \n",
            "4        0.955303   0.973485  \n",
            "\\begin{tabular}{lrrrrrrrr}\n",
            "\\toprule\n",
            "{} &       SVM &      Tree &  BaggingSVM &  BaggingTree &  Boosting1\\_SVM &  Boosting1\\_Tree &  Boosting2\\_Tree &  GradBoost \\\\\n",
            "\\midrule\n",
            "0 &  0.935470 &  0.847721 &    0.931624 &     0.893447 &       0.642735 &        0.837179 &        0.825499 &   0.908689 \\\\\n",
            "1 &  0.767302 &  0.718966 &    0.763733 &     0.729099 &       0.661494 &        0.731065 &        0.722293 &   0.755142 \\\\\n",
            "2 &  0.774454 &  0.917981 &    0.768180 &     0.941642 &       0.764980 &        0.921106 &        0.925843 &   0.955779 \\\\\n",
            "3 &  0.964919 &  0.981117 &    0.970324 &     0.987856 &       0.905532 &        0.981117 &        0.979748 &   0.987874 \\\\\n",
            "4 &  0.981818 &  0.946212 &    0.972727 &     0.946970 &       0.348485 &        0.955303 &        0.955303 &   0.973485 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compareDF = pd.DataFrame()\n",
        "compareDF['SVM'] = ScoreSVM\n",
        "compareDF['Tree'] = ScoreTree\n",
        "compareDF['BaggingSVM'] = ScoreBaggingSVM\n",
        "compareDF['BaggingTree'] = ScoreBaggingTree\n",
        "compareDF['Boosting1_SVM'] = ScoreBoosting1_SVM\n",
        "compareDF['Boosting1_Tree'] = ScoreBoosting1_Tree\n",
        "compareDF['Boosting2_Tree'] = ScoreBoosting2_Tree\n",
        "compareDF['GradBoost'] = ScoreGradBoost\n",
        "compareDF.to_csv(\"performance.csv\", index=False)\n",
        "print(compareDF)\n",
        "print(compareDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbtSJ63cLXGE",
        "outputId": "d3c9cae6-452e-43b8-a06a-4fa378e4e69c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def compute_iman_davenport_statistic(performance_matrix):\n",
        "  # Compute the ranks of the model performance on each dataset\n",
        "  ranks = np.apply_along_axis(lambda x: len(x) - np.argsort(np.argsort(x)), 1, performance_matrix)\n",
        "\n",
        "  # Sum the ranks for each model across all datasets\n",
        "  rank_sums = np.sum(ranks, axis=0)\n",
        "\n",
        "  # Compute the iman Davenport statistic\n",
        "  iman_davenport_statistic = (np.max(rank_sums) - np.min(rank_sums)) / performance_matrix.shape[1]\n",
        "\n",
        "  return iman_davenport_statistic\n",
        "\n",
        "def compute_p_value(iman_davenport_statistic, num_models, num_datasets):\n",
        "  # Compute the degrees of freedom for the iman Davenport test\n",
        "  df = num_models - 1\n",
        "\n",
        "  # Compute the p-value using the chi-squared distribution\n",
        "  p_value = 1 - chi2.cdf(iman_davenport_statistic, df)\n",
        "\n",
        "  return p_value\n",
        "\n",
        "def iman_davenport_test(performance_matrix, significance_level):\n",
        "  # Compute the iman Davenport statistic and p-value\n",
        "  iman_davenport_statistic = compute_iman_davenport_statistic(performance_matrix)\n",
        "  p_value = compute_p_value(iman_davenport_statistic, performance_matrix.shape[0], performance_matrix.shape[1])\n",
        "\n",
        "  # Determine whether the difference in performance between the models is statistically significant\n",
        "  if p_value < significance_level:\n",
        "    print(f\"The difference in performance between the models is statistically significant (p = {p_value:.3f})\")\n",
        "  else:\n",
        "    print(f\"The difference in performance between the models is not statistically significant (p = {p_value:.3f})\")\n",
        "  return p_value"
      ],
      "metadata": {
        "id": "mUvKBByaLXGE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The difference in performance between the models is not statistically significant (p = 0.441)\n"
          ]
        }
      ],
      "source": [
        "performance_matrix = pd.read_csv(\"performance.csv\")\n",
        "# Run the iman Davenport test\n",
        "p_value = iman_davenport_test(performance_matrix, 0.05)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRSPYJBtLXGF",
        "outputId": "696f3a37-bd0b-4ef0-ce95-b43d6bee7853"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "source": [
        "# If there are significant differences between the models, then apply the wilcoxon test to determine which models are significantly different\n",
        "if p_value < 0.05:\n",
        "  # Compute the pairwise differences between the models\n",
        "  pairwise_differences = np.apply_along_axis(lambda x: x - x[:, None], 1, performance_matrix)\n",
        "\n",
        "  # Compute the p-values for the pairwise differences using the Wilcoxon signed-rank test\n",
        "  p_values = np.apply_along_axis(lambda x: wilcoxon(x, zero_method=\"wilcox\")[1], 1, pairwise_differences)\n",
        "\n",
        "  # Compute the Bonferroni correction\n",
        "  bonferroni_correction = 0.05 / (p_values.shape[0] * (p_values.shape[0] - 1) / 2)\n",
        "\n",
        "  # Determine which models are significantly different\n",
        "  significant_differences = np.argwhere(p_values < bonferroni_correction)\n",
        "\n",
        "  # Print the significant differences\n",
        "  for difference in significant_differences:\n",
        "    print(f\"Model {difference[0]} is significantly different from model {difference[1]}\")"
      ],
      "metadata": {
        "id": "ScH4MxBOLXGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison using GridSearch"
      ],
      "metadata": {
        "collapsed": false,
        "id": "g2G5IVqcLXGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "source": [
        "parametersSVM = [\n",
        "    {\"kernel\": [\"linear\"], \"C\": [1, 10]}\n",
        "]\n",
        "parametersTree = {\n",
        "    'criterion':['gini', 'entropy', 'log_loss'],\n",
        "    'splitter' : ['best', 'random']\n",
        "}\n",
        "parametersSVMBagging = [\n",
        "    {\n",
        "    'n_estimators':[10], # Numero de estimators = 10 porque si no tarda demasiado\n",
        "    \"estimator__kernel\": [\"rbf\"],\n",
        "    \"estimator__C\": [0.01, 0.1],\n",
        "    'estimator__gamma': [0.01, 0.1],\n",
        "    #'max_samples': [0.75, 1]\n",
        "    # Saltan warnings si aleatoriamente solo seleccionamos instancias de una clase. Podríamos ignorar dichos fits o capturar los warning. Eliminamos el problema directamente\n",
        "    'max_features': [0.5, 0.75],\n",
        "    'bootstrap': [True, False]\n",
        "    }\n",
        "]\n",
        "parametersTreeBagging = {\n",
        "    'estimator__criterion':['gini', 'entropy', 'log_loss'],\n",
        "    'estimator__splitter' : ['best', 'random'],\n",
        "    # {'max_samples': [0.75, 1],\n",
        "    'max_features': [0.5, 0.75, 1],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "parametersSVMBoosting = [\n",
        "    {\n",
        "    \"estimator__kernel\": [\"rbf\"],\n",
        "    \"estimator__C\": [1, 10, 100, 1000],\n",
        "    'estimator__gamma': [0.01, 0.1, 1],\n",
        "    'n_estimators':[25, 50],\n",
        "    'learning_rate':[0.1, 1, 10]\n",
        "    }\n",
        "]\n",
        "parametersTreeBoosting = {\n",
        "    'estimator__criterion':['gini', 'entropy', 'log_loss'],\n",
        "    'estimator__splitter' : ['best', 'random'],\n",
        "    'n_estimators':[25, 50],\n",
        "    'learning_rate':[0.1,1,10]\n",
        "}\n",
        "parametersGradBoosting = {\n",
        "    \"loss\" : ['log_loss', 'deviance', 'exponential'],\n",
        "    'learning_rate' : [0.01, 0.1, 1],\n",
        "    'n_estimators': [100],\n",
        "    'criterion': ['friedman_mse', 'squared_error'],\n",
        "    'warm_start': [True, False]\n",
        "}\n",
        "\n",
        "optimalSVM = GridSearchCV(estimator=clf_svm, cv=10, param_grid=parametersSVM, n_jobs=-1)\n",
        "optimalTree = GridSearchCV(estimator=clf_tree, cv=10, param_grid=parametersTree, n_jobs=-1)\n",
        "optimalSVMBag = GridSearchCV(estimator=clf_BaggingSVM, cv=10, param_grid=parametersSVMBagging, n_jobs=-1)\n",
        "optimalTreeBag = GridSearchCV(estimator=clf_BaggingTree, cv=10, param_grid=parametersTreeBagging, n_jobs=-1)\n",
        "optimalSVMBoost1 = GridSearchCV(estimator=clf_AdaBoostSAMMESVM, cv=10, param_grid=parametersSVMBoosting, n_jobs=-1)\n",
        "optimalTreeBoost1 = GridSearchCV(estimator=clf_AdaBoostSAMMETree, cv=10, param_grid=parametersTreeBoosting, n_jobs=-1)\n",
        "optimalSVMBoost2 = GridSearchCV(estimator=clf_AdaBoostSAMMERSVM, cv=10, param_grid=parametersTreeBoosting, n_jobs=-1)\n",
        "optimalTreeBoost2 = GridSearchCV(estimator=clf_AdaBoostSAMMERTree, cv=10, param_grid=parametersTreeBoosting, n_jobs=-1)\n",
        "optimalGradBoost = GridSearchCV(estimator=clf_GradBoost, cv=10, param_grid=parametersGradBoosting, n_jobs=-1)"
      ],
      "metadata": {
        "id": "wHwmOeyuLXGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVM:\n",
            "    Best params -> {'C': 10, 'kernel': 'linear'}\n",
            "    Best score -> 0.8783475783475783\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.875\n",
            "Tiempo medio en ejecutarse el método: 0.013677060604095459 +- 0.0037599614108274806s\n",
            "Tiempo en ejecutarse la búsqueda 0.2499380111694336s, (0.00416563351949056 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVM:\n",
            "    Best params -> {'C': 10, 'kernel': 'linear'}\n",
            "    Best score -> 0.7776164549304296\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.75\n",
            "Tiempo medio en ejecutarse el método: 21.518423187732697 +- 5.785629552233049s\n",
            "Tiempo en ejecutarse la búsqueda 247.28827834129333s, (4.121471305688222 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVM:\n",
            "    Best params -> {'C': 10, 'kernel': 'linear'}\n",
            "    Best score -> 0.963640873015873\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9669811320754716\n",
            "Tiempo medio en ejecutarse el método: 0.8346085906028747 +- 0.662961396873663s\n",
            "Tiempo en ejecutarse la búsqueda 9.929230451583862s, (0.16548717419306438 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVM:\n",
            "    Best params -> {'C': 1, 'kernel': 'linear'}\n",
            "    Best score -> 0.9703423423423425\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9635627530364372\n",
            "Tiempo medio en ejecutarse el método: 0.028099501132965086 +- 0.008636645572718822s\n",
            "Tiempo en ejecutarse la búsqueda 0.45132899284362793s, (0.007522149880727132 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVM:\n",
            "    Best params -> {'C': 1, 'kernel': 'linear'}\n",
            "    Best score -> 0.9818181818181818\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9210526315789473\n",
            "Tiempo medio en ejecutarse el método: 0.0069678306579589845 +- 0.003940500849007087s\n",
            "Tiempo en ejecutarse la búsqueda 0.2052919864654541s, (0.0034215331077575685 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.013677 &    0.249938 &  0.875000 \\\\\n",
            "1 &    Diabetes &   21.518423 &  247.288278 &  0.750000 \\\\\n",
            "2 &     Vehicle &    0.834609 &    9.929230 &  0.966981 \\\\\n",
            "3 &       Vowel &    0.028100 &    0.451329 &  0.963563 \\\\\n",
            "4 &        Iris &    0.006968 &    0.205292 &  0.921053 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreSVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalSVM.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente SVM:\")\n",
        "    print(f'    Best params -> {optimalSVM.best_params_}')\n",
        "    print(f'    Best score -> {optimalSVM.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalSVM.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalSVM.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalSVM.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalSVM.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreSVM.append(optimalSVM.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreSVM)\n",
        "SVMDF = pd.DataFrame (my_dict)\n",
        "print(SVMDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oJNlg5XLXGF",
        "outputId": "e7a472e2-426b-4654-d1eb-6333b8a60e12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente Tree:\n",
            "    Best params -> {'criterion': 'entropy', 'splitter': 'random'}\n",
            "    Best score -> 0.9085470085470085\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9318181818181818\n",
            "Tiempo medio en ejecutarse el método: 0.013209176063537597 +- 0.004366503370763392s\n",
            "Tiempo en ejecutarse la búsqueda 0.6956453323364258s, (0.011594088872273763 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente Tree:\n",
            "    Best params -> {'criterion': 'gini', 'splitter': 'best'}\n",
            "    Best score -> 0.7241681790683606\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.65625\n",
            "Tiempo medio en ejecutarse el método: 0.005953383445739746 +- 0.0012378103965433468s\n",
            "Tiempo en ejecutarse la búsqueda 0.3425593376159668s, (0.005709322293599447 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente Tree:\n",
            "    Best params -> {'criterion': 'log_loss', 'splitter': 'best'}\n",
            "    Best score -> 0.9289930555555556\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9575471698113207\n",
            "Tiempo medio en ejecutarse el método: 0.005714408556620279 +- 0.0008693620755316155s\n",
            "Tiempo en ejecutarse la búsqueda 0.3369135856628418s, (0.00561522642771403 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente Tree:\n",
            "    Best params -> {'criterion': 'gini', 'splitter': 'random'}\n",
            "    Best score -> 0.9892072072072071\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9878542510121457\n",
            "Tiempo medio en ejecutarse el método: 0.006490743160247802 +- 0.0015604759391224879s\n",
            "Tiempo en ejecutarse la búsqueda 0.3563857078552246s, (0.005939761797587077 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente Tree:\n",
            "    Best params -> {'criterion': 'log_loss', 'splitter': 'random'}\n",
            "    Best score -> 0.9643939393939392\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9736842105263158\n",
            "Tiempo medio en ejecutarse el método: 0.0035713712374369298 +- 0.0011767970450761182s\n",
            "Tiempo en ejecutarse la búsqueda 0.24466753005981445s, (0.004077792167663574 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.013209 &    0.695645 &  0.931818 \\\\\n",
            "1 &    Diabetes &    0.005953 &    0.342559 &  0.656250 \\\\\n",
            "2 &     Vehicle &    0.005714 &    0.336914 &  0.957547 \\\\\n",
            "3 &       Vowel &    0.006491 &    0.356386 &  0.987854 \\\\\n",
            "4 &        Iris &    0.003571 &    0.244668 &  0.973684 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreTree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalTree.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente Tree:\")\n",
        "    print(f'    Best params -> {optimalTree.best_params_}')\n",
        "    print(f'    Best score -> {optimalTree.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalTree.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalTree.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalTree.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalTree.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreTree.append(optimalTree.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreTree)\n",
        "TreeDF = pd.DataFrame (my_dict)\n",
        "print(TreeDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzogW52tLXGG",
        "outputId": "b629b75d-05eb-47c7-d1ba-82bbe73b39d5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBag:\n",
            "    Best params -> {'bootstrap': True, 'estimator__C': 0.1, 'estimator__gamma': 0.1, 'estimator__kernel': 'rbf', 'max_features': 0.75, 'n_estimators': 10}\n",
            "    Best score -> 0.8709401709401708\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.8409090909090909\n",
            "Tiempo medio en ejecutarse el método: 0.06413227319717407 +- 0.005516524172943507s\n",
            "Tiempo en ejecutarse la búsqueda 6.35749363899231s, (0.10595822731653849 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBag:\n",
            "    Best params -> {'bootstrap': True, 'estimator__C': 0.01, 'estimator__gamma': 0.01, 'estimator__kernel': 'rbf', 'max_features': 0.5, 'n_estimators': 10}\n",
            "    Best score -> 0.6614942528735632\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.6197916666666666\n",
            "Tiempo medio en ejecutarse el método: 0.2152583345770836 +- 0.04198128183705442s\n",
            "Tiempo en ejecutarse la búsqueda 21.07009220123291s, (0.3511682033538818 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBag:\n",
            "    Best params -> {'bootstrap': True, 'estimator__C': 0.01, 'estimator__gamma': 0.01, 'estimator__kernel': 'rbf', 'max_features': 0.5, 'n_estimators': 10}\n",
            "    Best score -> 0.7649801587301587\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.7641509433962265\n",
            "Tiempo medio en ejecutarse el método: 0.2936517894268036 +- 0.033991740229249195s\n",
            "Tiempo en ejecutarse la búsqueda 28.08646011352539s, (0.4681076685587565 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBag:\n",
            "    Best params -> {'bootstrap': True, 'estimator__C': 0.01, 'estimator__gamma': 0.01, 'estimator__kernel': 'rbf', 'max_features': 0.5, 'n_estimators': 10}\n",
            "    Best score -> 0.9055315315315315\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9190283400809717\n",
            "Tiempo medio en ejecutarse el método: 0.10718133449554444 +- 0.012671122067670857s\n",
            "Tiempo en ejecutarse la búsqueda 10.443707466125488s, (0.17406179110209147 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBag:\n",
            "    Best params -> {'bootstrap': False, 'estimator__C': 0.1, 'estimator__gamma': 0.1, 'estimator__kernel': 'rbf', 'max_features': 0.75, 'n_estimators': 10}\n",
            "    Best score -> 0.9469696969696969\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.8157894736842105\n",
            "Tiempo medio en ejecutarse el método: 0.04201055616140366 +- 0.00399135630893817s\n",
            "Tiempo en ejecutarse la búsqueda 4.149539470672607s, (0.06915899117787679 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.064132 &    6.357494 &  0.840909 \\\\\n",
            "1 &    Diabetes &    0.215258 &   21.070092 &  0.619792 \\\\\n",
            "2 &     Vehicle &    0.293652 &   28.086460 &  0.764151 \\\\\n",
            "3 &       Vowel &    0.107181 &   10.443707 &  0.919028 \\\\\n",
            "4 &        Iris &    0.042011 &    4.149539 &  0.815789 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreBaggingSVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalSVMBag.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente SVMBag:\")\n",
        "    print(f'    Best params -> {optimalSVMBag.best_params_}')\n",
        "    print(f'    Best score -> {optimalSVMBag.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalSVMBag.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalSVMBag.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalSVMBag.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalSVMBag.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreBaggingSVM.append(optimalSVMBag.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreBaggingSVM)\n",
        "SVMBagDF = pd.DataFrame (my_dict)\n",
        "print(SVMBagDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8AB6F1QLXGG",
        "outputId": "6e8df0d6-447b-410d-9dee-f25464ecbe9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBag:\n",
            "    Best params -> {'bootstrap': False, 'estimator__criterion': 'gini', 'estimator__splitter': 'random', 'max_features': 0.75}\n",
            "    Best score -> 0.9357549857549857\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9659090909090909\n",
            "Tiempo medio en ejecutarse el método: 0.05579126278559367 +- 0.007111126282347554s\n",
            "Tiempo en ejecutarse la búsqueda 11.740135431289673s, (0.19566892385482787 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBag:\n",
            "    Best params -> {'bootstrap': True, 'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'max_features': 0.75}\n",
            "    Best score -> 0.7673018753781004\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.7083333333333334\n",
            "Tiempo medio en ejecutarse el método: 0.051435379187266046 +- 0.0064023410216155s\n",
            "Tiempo en ejecutarse la búsqueda 10.93658995628357s, (0.18227649927139283 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBag:\n",
            "    Best params -> {'bootstrap': False, 'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'max_features': 0.75}\n",
            "    Best score -> 0.9715773809523809\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9764150943396226\n",
            "Tiempo medio en ejecutarse el método: 0.04788902600606283 +- 0.0050889027235668s\n",
            "Tiempo en ejecutarse la búsqueda 10.304713487625122s, (0.17174522479375204 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBag:\n",
            "    Best params -> {'bootstrap': False, 'estimator__criterion': 'gini', 'estimator__splitter': 'random', 'max_features': 0.75}\n",
            "    Best score -> 0.9973153153153153\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9959514170040485\n",
            "Tiempo medio en ejecutarse el método: 0.04791548517015245 +- 0.004447506039156652s\n",
            "Tiempo en ejecutarse la búsqueda 10.206502199172974s, (0.17010836998621623 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBag:\n",
            "    Best params -> {'bootstrap': False, 'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'max_features': 0.75}\n",
            "    Best score -> 0.9818181818181818\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.8947368421052632\n",
            "Tiempo medio en ejecutarse el método: 0.039851568804846874 +- 0.006539833398576344s\n",
            "Tiempo en ejecutarse la búsqueda 8.742635726928711s, (0.14571059544881185 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.055791 &   11.740135 &  0.965909 \\\\\n",
            "1 &    Diabetes &    0.051435 &   10.936590 &  0.708333 \\\\\n",
            "2 &     Vehicle &    0.047889 &   10.304713 &  0.976415 \\\\\n",
            "3 &       Vowel &    0.047915 &   10.206502 &  0.995951 \\\\\n",
            "4 &        Iris &    0.039852 &    8.742636 &  0.894737 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreBaggingTree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalTreeBag.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente TreeBag:\")\n",
        "    print(f'    Best params -> {optimalTreeBag.best_params_}')\n",
        "    print(f'    Best score -> {optimalTreeBag.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalTreeBag.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalTreeBag.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalTreeBag.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalTreeBag.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreBaggingTree.append(optimalTreeBag.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreBaggingTree)\n",
        "TreeBagDF = pd.DataFrame (my_dict)\n",
        "print(TreeBagDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j65ebA1QLXGG",
        "outputId": "b0f36ee6-c1a7-4c6e-cb63-d71498c5751f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBoost1:\n",
            "    Best params -> {'estimator__C': 100, 'estimator__gamma': 0.1, 'estimator__kernel': 'rbf', 'learning_rate': 0.1, 'n_estimators': 25}\n",
            "    Best score -> 0.947008547008547\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9886363636363636\n",
            "Tiempo medio en ejecutarse el método: 0.4636915627453062 +- 0.07680047199395146s\n",
            "Tiempo en ejecutarse la búsqueda 181.99045038223267s, (3.033174173037211 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBoost1:\n",
            "    Best params -> {'estimator__C': 1000, 'estimator__gamma': 0.01, 'estimator__kernel': 'rbf', 'learning_rate': 1, 'n_estimators': 50}\n",
            "    Best score -> 0.668421052631579\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.6041666666666666\n",
            "Tiempo medio en ejecutarse el método: 1.6354723155498503 +- 0.4704846335806997s\n",
            "Tiempo en ejecutarse la búsqueda 633.9929716587067s, (10.566549527645112 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBoost1:\n",
            "    Best params -> {'estimator__C': 1000, 'estimator__gamma': 0.01, 'estimator__kernel': 'rbf', 'learning_rate': 0.1, 'n_estimators': 25}\n",
            "    Best score -> 0.8044642857142857\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.7924528301886793\n",
            "Tiempo medio en ejecutarse el método: 1.777566123008728 +- 0.506750608083461s\n",
            "Tiempo en ejecutarse la búsqueda 686.958898305893s, (11.449314971764883 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBoost1:\n",
            "    Best params -> {'estimator__C': 100, 'estimator__gamma': 0.1, 'estimator__kernel': 'rbf', 'learning_rate': 1, 'n_estimators': 25}\n",
            "    Best score -> 1.0\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 1.0\n",
            "Tiempo medio en ejecutarse el método: 1.9370213131109872 +- 0.46732822776335975s\n",
            "Tiempo en ejecutarse la búsqueda 748.1195759773254s, (12.468659599622091 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente SVMBoost1:\n",
            "    Best params -> {'estimator__C': 1000, 'estimator__gamma': 0.01, 'estimator__kernel': 'rbf', 'learning_rate': 0.1, 'n_estimators': 25}\n",
            "    Best score -> 0.9909090909090909\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9210526315789473\n",
            "Tiempo medio en ejecutarse el método: 0.12559474772877163 +- 0.031510185449383186s\n",
            "Tiempo en ejecutarse la búsqueda 50.64575791358948s, (0.8440959652264913 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.463692 &  181.990450 &  0.988636 \\\\\n",
            "1 &    Diabetes &    1.635472 &  633.992972 &  0.604167 \\\\\n",
            "2 &     Vehicle &    1.777566 &  686.958898 &  0.792453 \\\\\n",
            "3 &       Vowel &    1.937021 &  748.119576 &  1.000000 \\\\\n",
            "4 &        Iris &    0.125595 &   50.645758 &  0.921053 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreBoosting1_SVM = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalSVMBoost1.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente SVMBoost1:\")\n",
        "    print(f'    Best params -> {optimalSVMBoost1.best_params_}')\n",
        "    print(f'    Best score -> {optimalSVMBoost1.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalSVMBoost1.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalSVMBoost1.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalSVMBoost1.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalSVMBoost1.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreBoosting1_SVM.append(optimalSVMBoost1.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreBoosting1_SVM)\n",
        "SVMBoost1DF = pd.DataFrame (my_dict)\n",
        "print(SVMBoost1DF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gGt52sQLXGH",
        "outputId": "f4d3677a-f8b4-4edd-8a9d-7e8be570eb17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost1:\n",
            "    Best params -> {'estimator__criterion': 'log_loss', 'estimator__splitter': 'random', 'learning_rate': 10, 'n_estimators': 25}\n",
            "    Best score -> 0.9085470085470085\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9204545454545454\n",
            "Tiempo medio en ejecutarse el método: 0.010336774587631223 +- 0.0017734523519416026s\n",
            "Tiempo en ejecutarse la búsqueda 2.8045172691345215s, (0.04674195448557536 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost1:\n",
            "    Best params -> {'estimator__criterion': 'gini', 'estimator__splitter': 'best', 'learning_rate': 10, 'n_estimators': 25}\n",
            "    Best score -> 0.7275559588626741\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.6666666666666666\n",
            "Tiempo medio en ejecutarse el método: 0.010437184572219848 +- 0.002403138391451731s\n",
            "Tiempo en ejecutarse la búsqueda 2.887934923171997s, (0.04813224871953328 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost1:\n",
            "    Best params -> {'estimator__criterion': 'log_loss', 'estimator__splitter': 'random', 'learning_rate': 0.1, 'n_estimators': 25}\n",
            "    Best score -> 0.9416170634920634\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9622641509433962\n",
            "Tiempo medio en ejecutarse el método: 0.011080575651592678 +- 0.0026764394062245727s\n",
            "Tiempo en ejecutarse la búsqueda 3.2381222248077393s, (0.05396870374679565 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost1:\n",
            "    Best params -> {'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'learning_rate': 0.1, 'n_estimators': 50}\n",
            "    Best score -> 0.9919099099099098\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9878542510121457\n",
            "Tiempo medio en ejecutarse el método: 0.009066041972902084 +- 0.0015788599835471746s\n",
            "Tiempo en ejecutarse la búsqueda 2.5214147567749023s, (0.0420235792795817 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost1:\n",
            "    Best params -> {'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'learning_rate': 10, 'n_estimators': 25}\n",
            "    Best score -> 0.9727272727272727\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.8947368421052632\n",
            "Tiempo medio en ejecutarse el método: 0.006055844492382473 +- 0.0015433104358373046s\n",
            "Tiempo en ejecutarse la búsqueda 1.8428549766540527s, (0.030714249610900878 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.010337 &    2.804517 &  0.920455 \\\\\n",
            "1 &    Diabetes &    0.010437 &    2.887935 &  0.666667 \\\\\n",
            "2 &     Vehicle &    0.011081 &    3.238122 &  0.962264 \\\\\n",
            "3 &       Vowel &    0.009066 &    2.521415 &  0.987854 \\\\\n",
            "4 &        Iris &    0.006056 &    1.842855 &  0.894737 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreBoosting1_Tree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalTreeBoost1.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente TreeBoost1:\")\n",
        "    print(f'    Best params -> {optimalTreeBoost1.best_params_}')\n",
        "    print(f'    Best score -> {optimalTreeBoost1.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalTreeBoost1.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalTreeBoost1.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalTreeBoost1.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalTreeBoost1.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreBoosting1_Tree.append(optimalTreeBoost1.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreBoosting1_Tree)\n",
        "TreeBoost1DF = pd.DataFrame (my_dict)\n",
        "print(TreeBoost1DF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzOGWZQMLXGH",
        "outputId": "2adf74c2-e76a-4d4d-b5c0-cdb39c13edc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDatasets = []\\nTimeSearch = []\\nTimeMethod = []\\nScore = []\\nfor i in data:\\n    init = time.time()\\n    optimalSVMBoost2.fit(i[1], i[2])\\n    end = time.time()\\n    timeSearch = end - init\\n    print(f\"\\n--------- {i[0]} ---------\")\\n    print(f\"La mejor accuracy se obtuvo con el siguiente SVMBoost2:\")\\n    print(f\\'    Best params -> {optimalSVMBoost2.best_params_}\\')\\n    print(f\\'    Best score -> {optimalSVMBoost2.best_score_}\\')\\n\\n    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\\n    print(f\"    score = {optimalSVMBoost2.score(i[3], i[4])}\")\\n    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalSVMBoost2.cv_results_.get(\\'mean_fit_time\\'))} +- {np.mean(optimalSVMBoost2.cv_results_.get(\\'std_fit_time\\'))}s\")\\n    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\\n    Datasets.append(i[0])\\n    TimeMethod.append(np.mean(optimalSVMBoost2.cv_results_.get(\\'mean_fit_time\\')))\\n    TimeSearch.append(timeSearch)\\n    Score.append(optimalSVMBoost2.score(i[3], i[4]))\\n\\nd7 = dict(SVM_Boost2=Score)\\nmy_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=Score)\\nSVMBoost2DF = pd.DataFrame (my_dict)\\nprint(SVMBoost2DF.to_latex())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "\"\"\"\n",
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "Score = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalSVMBoost2.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente SVMBoost2:\")\n",
        "    print(f'    Best params -> {optimalSVMBoost2.best_params_}')\n",
        "    print(f'    Best score -> {optimalSVMBoost2.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalSVMBoost2.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalSVMBoost2.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalSVMBoost2.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalSVMBoost2.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    Score.append(optimalSVMBoost2.score(i[3], i[4]))\n",
        "\n",
        "d7 = dict(SVM_Boost2=Score)\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=Score)\n",
        "SVMBoost2DF = pd.DataFrame (my_dict)\n",
        "print(SVMBoost2DF.to_latex())\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "3rixhCvFLXGH",
        "outputId": "6c50b96c-dd3f-407e-dad4-afe52d293cf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost2:\n",
            "    Best params -> {'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 25}\n",
            "    Best score -> 0.9163817663817664\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9318181818181818\n",
            "Tiempo medio en ejecutarse el método: 0.010791830221811929 +- 0.0016796092941906557s\n",
            "Tiempo en ejecutarse la búsqueda 3.0393548011779785s, (0.050655913352966306 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost2:\n",
            "    Best params -> {'estimator__criterion': 'gini', 'estimator__splitter': 'best', 'learning_rate': 10, 'n_estimators': 25}\n",
            "    Best score -> 0.7329098608590441\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.703125\n",
            "Tiempo medio en ejecutarse el método: 0.012744296259350248 +- 0.00344316060557427s\n",
            "Tiempo en ejecutarse la búsqueda 3.578855514526367s, (0.059647591908772786 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost2:\n",
            "    Best params -> {'estimator__criterion': 'gini', 'estimator__splitter': 'random', 'learning_rate': 0.1, 'n_estimators': 25}\n",
            "    Best score -> 0.9415922619047619\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9433962264150944\n",
            "Tiempo medio en ejecutarse el método: 0.009555291467242771 +- 0.001941199442924363s\n",
            "Tiempo en ejecutarse la búsqueda 2.7470743656158447s, (0.045784572760264076 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost2:\n",
            "    Best params -> {'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'learning_rate': 0.1, 'n_estimators': 50}\n",
            "    Best score -> 0.9905405405405405\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9757085020242915\n",
            "Tiempo medio en ejecutarse el método: 0.008942030535803901 +- 0.001315106998055629s\n",
            "Tiempo en ejecutarse la búsqueda 2.4976155757904053s, (0.04162692626317342 min)\n",
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente TreeBoost2:\n",
            "    Best params -> {'estimator__criterion': 'entropy', 'estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 25}\n",
            "    Best score -> 0.9825757575757574\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9473684210526315\n",
            "Tiempo medio en ejecutarse el método: 0.006272886859046089 +- 0.001547800377527194s\n",
            "Tiempo en ejecutarse la búsqueda 1.9402565956115723s, (0.03233760992685954 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.010792 &    3.039355 &  0.931818 \\\\\n",
            "1 &    Diabetes &    0.012744 &    3.578856 &  0.703125 \\\\\n",
            "2 &     Vehicle &    0.009555 &    2.747074 &  0.943396 \\\\\n",
            "3 &       Vowel &    0.008942 &    2.497616 &  0.975709 \\\\\n",
            "4 &        Iris &    0.006273 &    1.940257 &  0.947368 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreBoosting2_Tree = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalTreeBoost2.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente TreeBoost2:\")\n",
        "    print(f'    Best params -> {optimalTreeBoost2.best_params_}')\n",
        "    print(f'    Best score -> {optimalTreeBoost2.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalTreeBoost2.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalTreeBoost2.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalTreeBoost2.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalTreeBoost2.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreBoosting2_Tree.append(optimalTreeBoost2.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreBoosting2_Tree)\n",
        "TreeBoost2DF = pd.DataFrame (my_dict)\n",
        "print(TreeBoost2DF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp3A8-OxLXGH",
        "outputId": "4a2e7134-bec5-43d3-a7cd-158a3dda8eae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Ionosphere ---------\n",
            "La mejor accuracy se obtuvo con el siguiente GradBoost:\n",
            "    Best params -> {'criterion': 'squared_error', 'learning_rate': 1, 'loss': 'exponential', 'n_estimators': 100, 'warm_start': True}\n",
            "    Best score -> 0.9279202279202279\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9545454545454546\n",
            "Tiempo medio en ejecutarse el método: 0.38358656962712606 +- 0.04183697571233041s\n",
            "Tiempo en ejecutarse la búsqueda 71.23071599006653s, (1.1871785998344422 min)\n",
            "\n",
            "--------- Diabetes ---------\n",
            "La mejor accuracy se obtuvo con el siguiente GradBoost:\n",
            "    Best params -> {'criterion': 'friedman_mse', 'learning_rate': 0.1, 'loss': 'exponential', 'n_estimators': 100, 'warm_start': True}\n",
            "    Best score -> 0.7638838475499092\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.7083333333333334\n",
            "Tiempo medio en ejecutarse el método: 0.2663808359040154 +- 0.02545372676082246s\n",
            "Tiempo en ejecutarse la búsqueda 49.71514868736267s, (0.8285858114560445 min)\n",
            "\n",
            "--------- Vehicle ---------\n",
            "La mejor accuracy se obtuvo con el siguiente GradBoost:\n",
            "    Best params -> {'criterion': 'friedman_mse', 'learning_rate': 1, 'loss': 'exponential', 'n_estimators': 100, 'warm_start': True}\n",
            "    Best score -> 0.9684275793650794\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.9528301886792453\n",
            "Tiempo medio en ejecutarse el método: 0.3874237298965454 +- 0.04622223138583237s\n",
            "Tiempo en ejecutarse la búsqueda 71.82525300979614s, (1.197087550163269 min)\n",
            "\n",
            "--------- Vowel ---------\n",
            "La mejor accuracy se obtuvo con el siguiente GradBoost:\n",
            "    Best params -> {'criterion': 'squared_error', 'learning_rate': 1, 'loss': 'exponential', 'n_estimators': 100, 'warm_start': True}\n",
            "    Best score -> 0.9905765765765766\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 1.0\n",
            "Tiempo medio en ejecutarse el método: 0.45411289334297184 +- 0.07042050650232466s\n",
            "Tiempo en ejecutarse la búsqueda 83.76822996139526s, (1.3961371660232544 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "120 fits failed out of a total of 360.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "120 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\", line 444, in fit\n",
            "    self._check_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\", line 301, in _check_params\n",
            "    self._loss = loss_class(self.n_classes_)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb_losses.py\", line 891, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: ExponentialLoss requires 2 classes; got 3 class(es)\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.94621212 0.94621212 0.94621212 0.94621212        nan        nan\n",
            " 0.97348485 0.97348485 0.97348485 0.97348485        nan        nan\n",
            " 0.95606061 0.95606061 0.96439394 0.96439394        nan        nan\n",
            " 0.94621212 0.94621212 0.94621212 0.94621212        nan        nan\n",
            " 0.97348485 0.97348485 0.97348485 0.97348485        nan        nan\n",
            " 0.95606061 0.95530303 0.95530303 0.95606061        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Iris ---------\n",
            "La mejor accuracy se obtuvo con el siguiente GradBoost:\n",
            "    Best params -> {'criterion': 'friedman_mse', 'learning_rate': 0.1, 'loss': 'log_loss', 'n_estimators': 100, 'warm_start': True}\n",
            "    Best score -> 0.9734848484848484\n",
            "Si usamos el dataset de test, obtenemos el siguiente resultado:\n",
            "    score = 0.8947368421052632\n",
            "Tiempo medio en ejecutarse el método: 0.2828403274218242 +- 0.030977160311430236s\n",
            "Tiempo en ejecutarse la búsqueda 52.46371912956238s, (0.8743953188260396 min)\n",
            "\\begin{tabular}{llrrr}\n",
            "\\toprule\n",
            "{} &     Dataset &  TimeMethod &  TimeSearch &     Score \\\\\n",
            "\\midrule\n",
            "0 &  Ionosphere &    0.383587 &   71.230716 &  0.954545 \\\\\n",
            "1 &    Diabetes &    0.266381 &   49.715149 &  0.708333 \\\\\n",
            "2 &     Vehicle &    0.387424 &   71.825253 &  0.952830 \\\\\n",
            "3 &       Vowel &    0.454113 &   83.768230 &  1.000000 \\\\\n",
            "4 &        Iris &    0.282840 &   52.463719 &  0.894737 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Datasets = []\n",
        "TimeSearch = []\n",
        "TimeMethod = []\n",
        "ScoreGradBoost = []\n",
        "for i in data:\n",
        "    init = time.time()\n",
        "    optimalGradBoost.fit(i[1], i[2])\n",
        "    end = time.time()\n",
        "    timeSearch = end - init\n",
        "    print(f\"\\n--------- {i[0]} ---------\")\n",
        "    print(f\"La mejor accuracy se obtuvo con el siguiente GradBoost:\")\n",
        "    print(f'    Best params -> {optimalGradBoost.best_params_}')\n",
        "    print(f'    Best score -> {optimalGradBoost.best_score_}')\n",
        "\n",
        "    print(f\"Si usamos el dataset de test, obtenemos el siguiente resultado:\")\n",
        "    print(f\"    score = {optimalGradBoost.score(i[3], i[4])}\")\n",
        "    print(f\"Tiempo medio en ejecutarse el método: {np.mean(optimalGradBoost.cv_results_.get('mean_fit_time'))} +- {np.mean(optimalGradBoost.cv_results_.get('std_fit_time'))}s\")\n",
        "    print(f\"Tiempo en ejecutarse la búsqueda {timeSearch}s, ({timeSearch/60} min)\")\n",
        "    Datasets.append(i[0])\n",
        "    TimeMethod.append(np.mean(optimalGradBoost.cv_results_.get('mean_fit_time')))\n",
        "    TimeSearch.append(timeSearch)\n",
        "    ScoreGradBoost.append(optimalGradBoost.score(i[3], i[4]))\n",
        "\n",
        "my_dict = dict(Dataset=Datasets,TimeMethod=TimeMethod,TimeSearch=TimeSearch, Score=ScoreGradBoost)\n",
        "GradBoostDF = pd.DataFrame (my_dict)\n",
        "print(GradBoostDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIIPncHgLXGI",
        "outputId": "bef7442c-9940-4c59-8b4e-1d69cf3f63db"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        SVM      Tree  BaggingSVM  BaggingTree  Boosting1_SVM  Boosting1_Tree  \\\n",
            "0  0.875000  0.931818    0.840909     0.965909       0.988636        0.920455   \n",
            "1  0.750000  0.656250    0.619792     0.708333       0.604167        0.666667   \n",
            "2  0.966981  0.957547    0.764151     0.976415       0.792453        0.962264   \n",
            "3  0.963563  0.987854    0.919028     0.995951       1.000000        0.987854   \n",
            "4  0.921053  0.973684    0.815789     0.894737       0.921053        0.894737   \n",
            "\n",
            "   Boosting2_Tree  GradBoost  \n",
            "0        0.931818   0.954545  \n",
            "1        0.703125   0.708333  \n",
            "2        0.943396   0.952830  \n",
            "3        0.975709   1.000000  \n",
            "4        0.947368   0.894737  \n",
            "\\begin{tabular}{lrrrrrrrr}\n",
            "\\toprule\n",
            "{} &       SVM &      Tree &  BaggingSVM &  BaggingTree &  Boosting1\\_SVM &  Boosting1\\_Tree &  Boosting2\\_Tree &  GradBoost \\\\\n",
            "\\midrule\n",
            "0 &  0.875000 &  0.931818 &    0.840909 &     0.965909 &       0.988636 &        0.920455 &        0.931818 &   0.954545 \\\\\n",
            "1 &  0.750000 &  0.656250 &    0.619792 &     0.708333 &       0.604167 &        0.666667 &        0.703125 &   0.708333 \\\\\n",
            "2 &  0.966981 &  0.957547 &    0.764151 &     0.976415 &       0.792453 &        0.962264 &        0.943396 &   0.952830 \\\\\n",
            "3 &  0.963563 &  0.987854 &    0.919028 &     0.995951 &       1.000000 &        0.987854 &        0.975709 &   1.000000 \\\\\n",
            "4 &  0.921053 &  0.973684 &    0.815789 &     0.894737 &       0.921053 &        0.894737 &        0.947368 &   0.894737 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compareDF = pd.DataFrame()\n",
        "compareDF['SVM'] = ScoreSVM\n",
        "compareDF['Tree'] = ScoreTree\n",
        "compareDF['BaggingSVM'] = ScoreBaggingSVM\n",
        "compareDF['BaggingTree'] = ScoreBaggingTree\n",
        "compareDF['Boosting1_SVM'] = ScoreBoosting1_SVM\n",
        "compareDF['Boosting1_Tree'] = ScoreBoosting1_Tree\n",
        "compareDF['Boosting2_Tree'] = ScoreBoosting2_Tree\n",
        "compareDF['GradBoost'] = ScoreGradBoost\n",
        "compareDF.to_csv(\"performanceGridSearch.csv\", index=False)\n",
        "print(compareDF)\n",
        "print(compareDF.to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox_MSQJULXGI",
        "outputId": "e1b66389-3006-4569-c98d-7eabc1d82155"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iman Davenport test"
      ],
      "metadata": {
        "collapsed": false,
        "id": "zYNHgj1oLXGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def compute_imon_davenport_statistic(performance_matrix):\n",
        "  # Compute the ranks of the model performance on each dataset\n",
        "  ranks = np.apply_along_axis(lambda x: len(x) - np.argsort(np.argsort(x)), 1, performance_matrix)\n",
        "\n",
        "  # Sum the ranks for each model across all datasets\n",
        "  rank_sums = np.sum(ranks, axis=0)\n",
        "\n",
        "  # Compute the iman Davenport statistic\n",
        "  iman_davenport_statistic = (np.max(rank_sums) - np.min(rank_sums)) / performance_matrix.shape[1]\n",
        "\n",
        "  return iman_davenport_statistic\n",
        "\n",
        "def compute_p_value(imon_davenport_statistic, num_models, num_datasets):\n",
        "  # Compute the degrees of freedom for the Imon Davenport test\n",
        "  df = num_models - 1\n",
        "\n",
        "  # Compute the p-value using the chi-squared distribution\n",
        "  p_value = 1 - chi2.cdf(imon_davenport_statistic, df)\n",
        "\n",
        "  return p_value\n",
        "\n",
        "def imon_davenport_test(performance_matrix, significance_level):\n",
        "  # Compute the Imon Davenport statistic and p-value\n",
        "  imon_davenport_statistic = compute_imon_davenport_statistic(performance_matrix)\n",
        "  p_value = compute_p_value(imon_davenport_statistic, performance_matrix.shape[0], performance_matrix.shape[1])\n",
        "\n",
        "  # Determine whether the difference in performance between the models is statistically significant\n",
        "  if p_value < significance_level:\n",
        "    print(f\"The difference in performance between the models is statistically significant (p = {p_value:.3f})\")\n",
        "  else:\n",
        "    print(f\"The difference in performance between the models is not statistically significant (p = {p_value:.3f})\")\n",
        "  return p_value"
      ],
      "metadata": {
        "id": "bqH18dK7LXGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The difference in performance between the models is not statistically significant (p = 0.579)\n"
          ]
        }
      ],
      "source": [
        "performance_matrix = pd.read_csv(\"performanceGridSearch.csv\")\n",
        "# Run the Imon Davenport test\n",
        "p_value = imon_davenport_test(performance_matrix, 0.05)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjnMdfGDLXGJ",
        "outputId": "30e74304-a186-4aa6-a636-e172abd077c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wilcoxon Test"
      ],
      "metadata": {
        "collapsed": false,
        "id": "wn-QaJ0HLXGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "source": [
        "# If there are significant differences between the models, then apply the wilcoxon test to determine which models are significantly different\n",
        "if p_value < 0.05:\n",
        "  # Compute the pairwise differences between the models\n",
        "  pairwise_differences = np.apply_along_axis(lambda x: x - x[:, None], 1, performance_matrix)\n",
        "\n",
        "  # Compute the p-values for the pairwise differences using the Wilcoxon signed-rank test\n",
        "  p_values = np.apply_along_axis(lambda x: wilcoxon(x, zero_method=\"wilcox\")[1], 1, pairwise_differences)\n",
        "\n",
        "  # Compute the Bonferroni correction\n",
        "  bonferroni_correction = 0.05 / (p_values.shape[0] * (p_values.shape[0] - 1) / 2)\n",
        "\n",
        "  # Determine which models are significantly different\n",
        "  significant_differences = np.argwhere(p_values < bonferroni_correction)\n",
        "\n",
        "  # Print the significant differences\n",
        "  for difference in significant_differences:\n",
        "    print(f\"Model {difference[0]} is significantly different from model {difference[1]}\")"
      ],
      "metadata": {
        "id": "hOtW3m5oLXGJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}